\chapter{\label{ch:experiments}Experiments}
In this chapter, we will describe the experiments we made and present the
results we obtained. In Section~\ref{sec:clone-detection-experiments}, we will
present our experiments about code clone detection, while in
Section~\ref{sec:token-generation-experiments} we will describe our experiments
to generate token embedding. In both sections we will show what kind of data we
used to train our model and how our model perform with different
hyper-parameters.

To run our experiments, we used the two machines shown in
table~\ref{tab:machine-specs}. In table~\ref{tab:softwares}, we show the
software we used to implement and run our machine-learning related experiments
and the version we used.

\begin{table}
  \caption{\label{tab:machine-specs}Machines used for experiments}
  \begin{center}
    \begin{tabular}{r c c}
      \toprule
       & Machine 1 & Machine 2\\
      \toprule
      \textbf{CPU} & Intel i7-6850K & Intel Xeon E5-2637 v3\\
      \textbf{CPU frequency} & 3.60GHz & 3.50GHz\\
      \textbf{Cores count} & 6 & 8\\
      \textbf{Threads count} & 12 & 16\\
      \textbf{Memory} & 64GB & 512GB\\
      \textbf{GPU} & Nvidia Quadro P6000 & Nvidia Tesla P100\\
      \textbf{GPU memory} & 24GB & 16GB\\
      \bottomrule
    \end{tabular}
  \end{center}
\end{table}
%
\begin{table}
  \caption{\label{tab:softwares}Software used for experiments}
  \begin{center}
    \begin{tabular}{r c}
      \toprule
       Software & Version\\
      \toprule
      Linux & 4.14 / 4.4\\
      CUDA & 8.0\\
      cuDNN & 6.0\\
      Python & 3.6.2\\
      Tensorflow & 1.3.1\\
      Keras & 2.1.2\\
      \bottomrule
    \end{tabular}
  \end{center}
\end{table}
\section{\label{sec:clone-detection-experiments}Code clone detection}
To evaluate our clone detection model, we implemented the model we described
in~\ref{sec:clone-detection}, we collected data of programs implementing the
same functionality to feed to the model, and we trained and evaluated our model
with different hyper-parameters. In this section, we will give some details
about the data we collected for our experiments and present the different
results we obtained when evaluating our model.
\subsection{\label{ssec:clone-detection-dataset}Code clone detection dataset}
As we are using a supervised learning approach to code clone detection, to train
our model we needed data which fulfills the following properties.

\begin{enumerate}
\item Multiple code fragments should implement the same functionality
\item Information on whether two code fragments implement the same functionality
  must be included
\item Dataset should contain code fragments written in at least 2 programming
  languages
\end{enumerate}

To the best of our knowledge, no dataset currently available fulfills all the
necessary properties to our experiments, therefore we created our own dataset.

For this dataset, we found that competitive programming websites are an
excellent match. The solution to a single problem is implemented by a
large number of persons in many different languages. All the solutions to a
single problem must implement exactly the same functionality, therefore,
we are assured that all source codes implementing a solution to the same problem
are at least type IV code clones. The easier the problem is, the higher the
probability of code fragments implementing the solution to the same problem have
to be very similar to each other, and to therefore be closer to type III clones.
Furthermore, multiple solutions to a problem are always implemented by different
users, which makes our dataset closer to the motivating example we presented in
Section~\ref{sec:motivating-exmaple}.

To create the dataset, we scraped code from
AtCoder\footnote{http://atcoder.jp/}, a famous Japanese competitive
programming website. As our implementation currently only supports Java and
Python, we fetched data for these two programming languages. We restricted the
data to only programs that were accepted by the website judging system --- meaning
that the programs actually implemented the solution to the given problem --- in
order to have the type IV code clone guarantee. In
table~\ref{tab:clone-dataset-overview}, we give some statistics about the
dataset we created and in table~\ref{tab:clone-dataset-languages} we show how
the given statistics are distributed between Python and Java. We show the
distribution of the number of files per problem --- which is representative of
the number of clones in the dataset --- in
figure~\ref{fig:files-per-problem-distribution}.

\begin{table}
  \caption{\label{tab:clone-dataset-overview} Clone detection dataset overview}
  \begin{center}
    \begin{tabular}{c c}
      \toprule
      Measure & Value\\
      \toprule
      Problems count & 576\\
      Avg. files / problem & 77\\
      Files count & 44620\\
      Lines count & 1270599\\
      Tokens count & 8554476\\
      \bottomrule
    \end{tabular}
  \end{center}
\end{table}

\begin{table}
  \caption{\label{tab:clone-dataset-languages} Clone detection dataset language overview}
  \begin{center}
    \begin{tabular}{c c c}
      \toprule
      & Python & Java\\
      \toprule
      Avg. files / problem & 41 & 36\\
      Files count & 23792 & 20828\\
      Lines count & 312353 & 958246\\
      Tokens count & 1810085 & 6744391\\
      \bottomrule
    \end{tabular}
  \end{center}
\end{table}

\begin{figure}
  \begin{center}
    \includegraphics[width=16cm]{./images/code-clone-dataset-problems-distribution.png}
    \caption{\label{fig:files-per-problem-distribution} Distribution of the
      number of files per problem}
  \end{center}
\end{figure}

In order to be able to feed the code to our model, we transform the code we
fetched using \lstinline{bigcode-astgen} we presented in
Section~\ref{sec:ast-generation} using the commands shown in
listing~\ref{lis:atcoder-astgen}.

\begin{lstlisting}[numbers=none,language={},
  caption=AST generation using \lstinline{bigcode-astgen}, label=lis:atcoder-astgen]
bigcode-astgen-java --batch -f 'src/**/*.java' -o java-asts
bigcode-astgen-py --batch -f 'src/**/*.py' -o python-asts
cat java-asts.json python-asts.json > asts.json
cat java-asts.txt python-asts.txt > asts.txt
\end{lstlisting}
With the first two commands, we generate the ASTs for Python and Java source
files, and with the next we commands we merge the ASTs and the file which maps
the index of the AST in the JSON file to the name of the file in the original
dataset.
\subsection{\label{ssec:clone-detection-hyper-params}Code clone detection model hyper-parameters}
Our model contains many hyper-parameters that can vastly influence its
performances while detecting clones. In listing~\ref{lis:model-config}, we show a
sample configuration file that we actually use to train our model. The file is
written using in YAML and is loaded by our system when training and evaluating
the model. Although the file we show here is not complete, it contains the main
settings that one might want to change when training a model.

\begin{figure}
  \lstinputlisting[language={}, basicstyle=\ttfamily\footnotesize, caption=Model
  configuration file,label=lis:model-config]{./snippets/config.yml}
\end{figure}

We describe the meaning of each parameter in our configuration file in
table~\ref{tab:hyper-parameters}.

\begin{table}[p]
  \caption{\label{tab:hyper-parameters}Clone detection model hyper-parameters}
  \begin{center}
    \begin{tabularx}{\linewidth}{c c X l}
      \toprule
      Parameter & Notation & Description & Sample value\\
      \toprule
      \lstinline{transformer_class_name} & $f_t$ & The algorithm to used to encode
      the AST into a vector as described in~\ref{sssec:ast-transformer} & \lstinline{DFSTransformer}\\
      \lstinline{vocabulary} & & The path to the extracted vocabulary & \lstinline{vocab.tsv}\\
      \lstinline{vocabulary_size} & $|V|$ & The size of the vocabulary & $500$\\
      \lstinline{embeddings} & $w$ & The path to the trained token embedding. If
      this parameter is not provided, embedding are randomly initialized & \lstinline{embeddings.pickle}\\
      \lstinline{embeddings_dimension} & $d_w$ & Dimension of the embedding for
      target language & $100$\\
      \lstinline{output_dimensions} & $d_e$ & Dimension of the
      LSTM encoder. If the size is greater than $1$, the LSTM will be stacked
      and the output dimension will be the value of the last element of the list
      & \lstinline{[100, 50]}\\
      \lstinline{bidirectional_encoding} & $f_{e_b}$ & Whether or not to use a
      bidirectional LSTM & \lstinline{true}\\
      \lstinline{name} & & The name of the programming language & java\\
      \lstinline{input_length} & $|X|$ & The max. number of tokens per code
      fragment. This restriction can be avoided at the cost of a performance
      penalty & $1000$\\
      \lstinline{merge_mode} & $f_m$ & Algorithm to use to merge the two code
      fragments as described in~\ref{sssec:merge-layer} & bidistance
    \end{tabularx}
  \end{center}
\end{table}
\begin{table}[p]
  \begin{center}
    \begin{tabularx}{\textwidth}{c c X l}
      \lstinline{merge_output_dim} & $d_o$ & Output dimension to use for the
      merge layer. Currently only effective when using
      equation~\ref{eq:bidistance-merge-layer} as the merge layer & \lstinline{50}\\
      \lstinline{dense_layers} & $d(f_p)$ & The number of hidden layers to use for the
      final MLP and their number of units & \lstinline{[50, 50]}\\
      \lstinline{optimizer} & & The optimizer to use to train the model & rmsprop\\
      \lstinline{submissions_path} & & The path of the submissions metadata & \lstinline{submissions.json}\\
      \lstinline{asts_path} & & The path of the training data ASTs generated
      as described above & \lstinline{asts.json}\\
      \lstinline{split_ratio} & & The amount of data to use for training,
      hyper-parameter tuning, and testing the model & \lstinline{[0.8, 0.1, 0.1]}\\
      \lstinline{shuffle} & & Whether or not to shuffle the data when training
      the model & \lstinline{true}\\
      \lstinline{negative_samples} & $|S_n|$ & The number of non-clone noise data ---
      negative sample --- to generate per code clone & $4$\\
      \lstinline{negative_sample_distance} & $\Delta S_n$ & The maximum distance ratio
      between the size of the input and the size of the negative sample, to avoid
      the model to try to detect clones by number of tokens & $0.2$\\
      \lstinline{class_weights} & $L_w$ & The weight to assign to positive and
      negative samples. The higher the weight, the more an error will be
      penalized & \lstinline{\{0: 2.0, 1: 1.0\}}\\
      \lstinline{batch_size} &  & The size of a batch when training the model & $256$\\
      \lstinline{epochs} & & The number of epochs to train the model & $10$\\
      \bottomrule
    \end{tabularx}
  \end{center}
\end{table}
%
Although there is a very large number of hyper-parameters, not all affect our
model in the same way. For example, while the vocabulary we use, or the way we
transform the AST into a vector affects greatly the performance of the model,
other parameters such as the optimizer we use or the batch size do not are not
as important. We will explain more in detail how each section in our
configuration is used, and explain some of the decisions to be made when setting
these parameters.

The parameters under each object of the \lstinline{languages} section contains
the necessary information to encode an AST into a vector for the given
programming language. The vocabulary and embedding we pass will determine how
each token is encoded into its vector representation. We will discuss further
how these embedding are trained in
Section~\ref{sec:token-generation-experiments}, but the most important decision
being made for the embedding is if we want to use the values of the tokens or
simply their types, resulting in a small vocabulary in the order of $100$ tokens
in the first case, or a much larger vocabulary of thousands of tokens in the
second. The input length is the maximum number of tokens for a single code
fragment. We use this limit mainly for performance purpose, as we pad all the
inputs to this size to be able to train our model using batches. Given our
current implementation, removing this limits forces us to train the model a
sample at a time, largely hurting performance. This shortcoming can be mitigated
with techniques such as bucketing~\cite{DBLP:journals/corr/abs-1708-05604}, but
we leave this for future work. The \lstinline{output_dimensions} and
\lstinline{bidirectional_encoding} parameters control what kind of LSTM network
will be used to encode the vectorized AST into a single vector.

Other parameters under \lstinline{model} contain hyper-parameters on how the
input vectors should be merged, as well as what kind of network to use to
actually make the prediction. It also contains information about the optimizer
to use, and may have hyper-parameters for the optimizer such as its learning
rate.

The part under \lstinline{generator} contains all the information necessary to
generate data to train the model. \lstinline{submissions_path} and
\lstinline{asts_path} are the paths to the submissions metadata and their
encoded AST and all the data to train and evaluate the model will be loaded from
their. The negative samples related settings as well as the class weights mostly
affect the precision-recall trade-off, which we will discuss more in detail
in~\ref{ssec:clone-detection-experiment}.
\subsection{\label{ssec:clone-detection-experiment}Clone detection experiment}
In this section, we will describe the experiments we run to evaluate our clone
detection model.
We will here describe the experiment we run, show the results and give an
interpretation of the results we obtained.

First, we prepared the dataset to train our model by splitting the data we
described in~\ref{ssec:clone-detection-dataset} into training set containing
80\% of the data, the cross-validation set used to tune our hyper parameters
containing 10\% of the data, and finally the test set used to give a final
evaluation of our model. For both training, cross-validation and test, we treat
files implementing a solution to the same problem as clones, and randomly choose
$n$ samples from files implementing a solution to a different problem to use as
negative inputs to our model. We make the number of samples vary during training
but fix this number to $4$ samples for cross-validation, in order to be able to
compare the performance of the different models on the same input data.

We trained models to detect clone between Java and Python source code, as well
as models to perform clone detection between source fragments written in Java.
The main motivation for trying to run clone detection is Java is to be able to
compare our results with other clone detection tools. In
tables~\ref{tab:java-python-model-params} and~\ref{tab:java-java-model-params},
we show the hyper parameters we used for training models for respectively
Java/Python code clone detection and Java/Java code clone detection. Given the
large number of parameters which we tried when training the models, we only show
a subset including models which yielded the best results, as wee as models which
we would like to discuss further in the results interpretation. Our models are
sorted using the F1-score. In tables~\ref{tab:java-python-model-results}
and~\ref{tab:java-java-model-results}, we show different metrics for each model
when tested on the cross-validation set.

\begin{table}
  \caption{\label{tab:java-python-model-params}Java/Python clone detection
    models parameters}
  \begin{center}
    \begin{tabularx}{\linewidth}{c Y c Y c Y c c Y Y c c}
      \toprule
      $n$ & $|V|$ & $w$ & $d_w$ & $d_e$ & $f_{e_b}$ & $d(f_p)$ & $f_t$ & $|S_n|$ & $L_w$ & $f_m$ & $\Delta S_n$\\
      \toprule
      $1$ & $10000$ & $p$ & $100$ & $\{100,50\}$ & $t$ & $\{64\}$ & $f_{t_1}$ & $4$ & $\{1.0,1.0\}$ & $f_{m_2}$ & $0.2$\\
      $2$ & $10000$ & $p$ & $100$ & $\{50,50\}$ & $t$ & $\{64\}$ & $f_{t_1}$ & $4$ & $\{1.0,1.0\}$ & $f_{m_2}$ & $0.2$\\
      $3$ & $10000$ & $r$ & $100$ & $\{100,50\}$ & $t$ & $\{64\}$ & $f_{t_1}$ & $4$ & $\{1.0,1.0\}$ & $f_{m_2}$ & $0.2$\\
      $4$ & $10000$ & $p$ & $100$ & $\{50\}$ & $t$ & $\{64\}$ & $f_{t_1}$ & $4$ & $\{1.0,1.0\}$ & $f_{m_2}$ & $0.2$\\
      $5$ & $-$ & $p$ & $50$ & $\{100,50\}$ & $t$ & $\{64\}$ & $f_{t_1}$ & $4$ & $\{2.0,1.0\}$ & $f_{m_2}$ & $0.2$\\
      \bottomrule
    \end{tabularx}
  \end{center}
\end{table}
%
\begin{table}
  \caption{\label{tab:java-python-model-results}Java/Python clone detection
    models results}
  \begin{center}
    \begin{tabularx}{\linewidth}{c Y Y Y Y}
      \toprule
      $n$ & F1-score & Precision & Recall & Accuracy\\
      \toprule
      $1$ & $0.66$ & $0.55$ & $0.83$ & $0.83$\\
      $2$ & $0.63$ & $0.51$ & $0.80$ & $0.80$\\
      $3$ & $0.61$ & $0.49$ & $0.82$ & $0.79$\\
      $4$ & $0.56$ & $0.42$ & $0.82$ & $0.73$\\
      $5$ & $0.51$ & $0.40$ & $0.71$ & $0.73$\\
      \bottomrule
    \end{tabularx}
  \end{center}
\end{table}
%
\begin{table}
  \caption{\label{tab:java-java-model-params}Java/Java clone detection
    models parameters}
  \begin{center}
    \begin{tabularx}{\linewidth}{c Y c Y c Y c c Y Y c c}
      \toprule
      $n$ & $|V|$ & $w$ & $d_w$ & $d_e$ & $f_{e_b}$ & $d(f_p)$ & $f_t$ & $|S_n|$ & $L_w$ & $f_m$ & $\Delta S_n$\\
      \toprule
      $1$ & $10000$ & $p$ & $100$ & $\{100,50\}$ & $t$ & $\{64\}$ & $f_{t_1}$ & $4$ & $\{1.0,1.0\}$ & $f_{m_2}$ & $0.2$\\
      $2$ & $10000$ & $p$ & $100$ & $\{50\}$ & $t$ & $\{64\}$ & $f_{t_1}$ & $4$ & $\{1.0,1.0\}$ & $f_{m_2}$ & $0.2$\\
      $3$ & $10000$ & $r$ & $100$ & $\{100,50\}$ & $t$ & $\{64\}$ & $f_{t_1}$ & $4$ & $\{1.0,1.0\}$ & $f_{m_2}$ & $0.2$\\
      $4$ & $10000$ & $p$ & $30$ & $\{50\}$ & $t$ & $\{64\}$ & $f_{t_1}$ & $4$ & $\{1.5,1.0\}$ & $f_{m_2}$ & $0.2$\\
      $5$ & $-$ & $p$ & $50$ & $\{100,50\}$ & $t$ & $\{64\}$ & $f_{t_1}$ & $4$ & $\{1.0,1.0\}$ & $f_{m_2}$ & $0.2$\\
      \bottomrule
    \end{tabularx}
  \end{center}
\end{table}
%
\begin{table}
  \caption{\label{tab:java-java-model-results}Java/Java clone detection
    models results}
  \begin{center}
    \begin{tabularx}{\linewidth}{c Y Y Y Y}
      \toprule
      $n$ & F1-score & Precision & Recall & Accuracy\\
      \toprule
      $1$ & $0.77$ & $0.67$ & $0.92$ & $0.89$\\
      $2$ & $0.76$ & $0.67$ & $0.88$ & $0.89$\\
      $3$ & $0.74$ & $0.65$ & $0.85$ & $0.88$\\
      $4$ & $0.72$ & $0.64$ & $0.82$ & $0.87$\\
      $5$ & $0.69$ & $0.56$ & $0.90$ & $0.84$\\
      \bottomrule
    \end{tabularx}
  \end{center}
\end{table}

In tables~\ref{tab:java-python-model-params}
and~\ref{tab:java-java-model-params}, having $-$ in the $|V|$ column means that
we did not use the values of the tokens, and we did not have to limit the size
of the vocabulary as it was already small enough. Concretely, the vocabulary
size was of $130$ for Python and of $75$ for Java.
The $w$ column shows what embeddings have been used when training the model.
$p$ means that pre-trained embeddings have been used, while $r$ means that
embeddings have been initialized randomly.

In general, our model gives us a high recall but does not manage to get a high
precision, especially for the cross-language classification task. These results
can be mitigated by trading recall for precision, but given the goal of our
model to find possible duplicates across systems, we prefer to keep a high
recall and leave the false-positive judgment to the developer. Furthermore, some
source code which are actually not clones do look very similar, and can
therefore be difficult to classify. For example, a code implementing Fibonacci
and a code implementing factorial both contain a main for loop, which does some
arithmetic operation and finally return an integer. Although the codes are not
doing exactly the same thing, and are therefore not clones, they contain enough
similarities to possibly be factorized, so the evaluation here could be a little
pessimistic.

From the hyper-parameters point of view, the main conclusion to draw from the
results we obtained is that when we do not include the values of the tokens and
try to train our model with only the types, the resulting model does not contain
enough information to be able to correctly detect clones and we have a large
drop in the precision score. However, especially for the single-language cross
detection, the precision is still promising, showing that the model somehow
manages to encode the structure of the AST and is not only mapping identifiers.

We can also see from these results that our pre-trained embeddings gives us a
performance improvement compared to randomly initialized embeddings, especially
in the task of cross-language clone detection where finding a relation between
different tokens is harder for the model.

Another conclusion that we can draw from our results is that increasing the size
of the LSTM encoder improves the performance of the model. Especially for
cross-language detection, where the detection task is more difficult, we get a
considerable improvement by using a stacked LSTM instead of a simple one.
On the other hand, using algorithm $f_{t_2}$, which performs two depth-first
search, for transforming our ASTs into vectors yielded performance much worse
that the simple $f_{t_1}$ which only perform a single one. We suspect that
adding the result of the second depth-first search to the vector encoding added
more noise to the encoded data that it added information, but a more formal
conclusion about this result needs further investigation. We did not observe any
significant improvement by changing the size of the output MLP classifier, and
therefore concluded that the model performance was rather dependent on how we
encode the AST into a vector rather the classification of the encoded vectors. 
\subsection{Comparison with existing methods}
We chose to compare our method to
SourcerCC\cite{Sajnani:2016:SSC:2884781.2884877}, as it is the tool published
the most recently with its source code publicly available. In the above
experiment, we sampled 4 non-clones code pairs for each clone pair when
evaluating our model. However, clone detection tools normally do not work by
taking code pairs but search for clones in a set of code fragments. As we
discuss in Section~\ref{sec:future-work}, our tool LearnerCC currently only
works by taking code fragments pairs as input, and we therefore need to run
$\mathcal{O}\left( n^2 \right)$ comparisons to detect clones in the same way as
the tool we are comparing our results to.

To run our experiment, we used the files from our test dataset, and run
our tools using all the pairs combinations, as well as SourcererCC on the
sampled files to detect clones within these files.

We will first describe the steps and settings we used to run the experiment
using our model. To run the experiment, we select the model which performed
the best in the above experiment according to the results presented in
table~\ref{tab:java-java-model-results}. We generate all the possible code
pairs of the files in our test set, and use our model to predict whether each
of these pairs were clones or not. Using our predictions, which are
probability-like values between $0$ and $1$, we set multiple thresholds to
whether the clone pair is a code clone are not. If the threshold is closer to
$0$, we accept pairs that are less likely to be clones and therefore trade
precision for recall, on the other hand if the threshold is closer to $0.9$, we
are stricter for which pairs to accept and therefore trade recall for precision.
We try a range of values for this threshold and compute the F1-score, precision
and recall for each of them. We show our results in
table~\ref{tab:clone-detection-results}.

\begin{table}
  \caption{\label{tab:clone-detection-results}Clone detection results}
  \begin{center}
    \begin{tabular}{c c c c}
      \toprule
      Threshold & F1-score & Precision & Recall\\
      \toprule
      $0.1$ & $0.13$ & $0.07$ & $0.98$\\
      $0.2$ & $0.15$ & $0.08$ & $0.97$\\
      $0.3$ & $0.17$ & $0.09$ & $0.95$\\
      $0.4$ & $0.18$ & $0.10$ & $0.92$\\
      $0.5$ & $0.19$ & $0.11$ & $0.89$\\
      \rowcolor{verylightgray} $0.6$ & $0.21$ & $0.12$ & $0.85$\\
      \rowcolor{verylightgray} $0.7$ & $0.22$ & $0.13$ & $0.77$\\
      \rowcolor{verylightgray} $0.8$ & $0.24$ & $0.14$ & $0.63$\\
      $0.9$ & $0.22$ & $0.16$ & $0.33$\\
      \bottomrule
    \end{tabular}
  \end{center}
\end{table}

After running the experiment with our model, we use the same dataset to run the
experiment with SourcererCC.
We show the parameters we used to run SourcererCC, which are the defaults at the
time of the experiment, in table~\ref{tab:sourcerer-cc-params}.

\begin{table}
  \caption{\label{tab:sourcerer-cc-params}SourcererCC settings}
  \begin{center}
  \begin{tabularx}{0.4\textwidth}{R R}
    \toprule
    Parameter & Value\\
    \toprule
    \lstinline{MIN_TOKENS} & $65$\\
    \lstinline{MAX_TOKENS} & $500000$\\
    \lstinline{BTSQ_THREADS} & $4$\\
    \lstinline{BTIIQ_THREADS} & $4$\\
    \lstinline{QLQ_THREADS} & $4$\\
    \lstinline{QBQ_THREADS} & $4$\\
    \lstinline{QCQ_THREADS} & $4$\\
    \lstinline{VCQ_THREADS} & $16$\\
    \lstinline{RCQ_THREADS} & $4$\\
    \bottomrule
  \end{tabularx}
  \end{center}
\end{table}

SourcererCC uses a threshold to decide whether it considers two code fragments
to be clones or not. If the value of the threshold is $0.5$, it means that at
least 50\% of the tokens must be shared between the code fragments to be
considered as clones. The higher the threshold, the more the code fragments need
to be similar to be considered as clones, therefore when the threshold becomes
higher, the precision increases and the recalls drop. We show in
table~\ref{tab:sourcerer-cc-results} the values we obtained for different values
of this threshold.

\begin{table}
  \caption{\label{tab:sourcerer-cc-results}SourcererCC clone detection results}
  \begin{center}
    \begin{tabular}{c c c c}
      \toprule
      Threshold & F1-score & Precision & Recall\\
      \toprule
      $0.3$ & $0.05$ & $0.03$ & $0.29$\\
      $0.4$ & $0.08$ & $0.05$ & $0.24$\\
      \rowcolor{verylightgray} $0.5$ & $0.12$ & $0.10$ & $0.15$\\
      $0.6$ & $0.11$ & $0.24$ & $0.07$\\
      $0.7$ & $0.05$ & $0.63$ & $0.03$\\
      $0.8$ & $0.02$ & $0.79$ & $0.01$\\
      $0.9$ & $0.01$ & $1.00$ & $0.00$\\
      \bottomrule
    \end{tabular}
  \end{center}
\end{table}

In table~\ref{tab:clone-detection-full-results}, we choose the results for each
tool which we think might be the most useful for the tool user --- not
necessarily the highest F1-score, but at least a good precision or recall ---
and compare them. In figure~\ref{fig:clone-detection-experiment}, we plot the
precision-recall curve with the results of both our methods and SourcererCC. The
x-axis is the recall and the y-axis is the precision, which means that we would
like to maximize the area under the curve.

\begin{table}
  \caption{\label{tab:clone-detection-full-results}Clone detection results comparison}
  \begin{center}
    \begin{tabular}{c c c c}
      \toprule
       & F1-score & Precision & Recall\\
      \toprule
      Our method & $0.21$ & $0.12$ & $0.85$\\
      SourcererCC & $0.05$ & $0.63$ & $0.03$\\
      \bottomrule
    \end{tabular}
  \end{center}
\end{table}

\begin{figure}
  \centering\includegraphics[width=14cm]{images/pr-curve.png}
  \caption{\label{fig:clone-detection-experiment}Precision-recall curve
    for clone detection experiment}
\end{figure}

Our method and SourcererCC have very different characteristics and are therefore
difficult to actually compare --- our method has a very high recall but a poor
precision, while SourcererCC has a good precision but a low recall. We will
here try to give some explanation about the results we obtained. Our dataset is
mostly composed of types IV clones, with probably only a very small amount of
clones which would enter the type III clone category. SourcererCC is a
token-based clone detection tool and is mostly design to detect type III clones,
and given the nature of its algorithm should be particularly efficient for
clones generated from copy-pasting. We believe that the poor results of
SourcererCC on this dataset comes from its nature --- clones in the dataset
being introduced by different programmers implementing the same functionality,
which generate very different types of clones. We think that the poor precision
results with our approach comes from two different factors. The main important
cause for this issue is that our tool is currently only able to predict if two
code fragment are clones are not, and to be able to run this experiment, we
therefore need to predict $\mathcal{O}\left( n^2 \right)$ samples. The amount of
clones in these samples is therefore less than 1\%, which vastly lower than the
20\% of clones we were using when training the model. This change in the
training and test distributions therefore tends to wrongly predict two code
pairs to be code clones. We describe more in detail in
Section~\ref{sec:future-work} a possible approach around this issue. Another
reason for the precision to be low is the nature of clone detection itself. We
currently mark two code fragments as clones only if they implement the exact
same functionality. However, some code pairs can have a very close structure,
which would make them good candidates for clones, but do not implement the same
functionality. We show such an example in listings~\ref{lis:wrong-clone}. Both
code read an integer from the standard input, declare other integers, execute
some simple arithmetic operations on the declared integer and finally output the
the result. Both codes are therefore doing something quite similar, but do not
enter in our current definition of code clones. However, the code fragments are
similar enough to be predicted as clones by our model, which lowers the
precision we obtain.
\begin{figure}
  % src/b/40/1/1068576.java
  \lstinputlisting[basicstyle=\ttfamily\footnotesize]{./snippets/WrongClone1.java}
  % src/b/56/2/1172809.java
  \lstinputlisting[label=lis:wrong-clone,caption=Structurally close code pair
  classified as clones,basicstyle=\ttfamily\footnotesize]{./snippets/WrongClone2.java}
\end{figure}
\section{\label{sec:token-generation-experiments}Token embedding generation}
To generate token vector embedding and evaluate our results, we implemented the
model and tools we described in Section~\ref{sec:token-representation}. All the
tools implemented to generated token embedding are publicly available on
GitHub\footnote{\url{https://github.com/tuvistavie/bigcode-tools}}, and made
available through pip when
possible\footnote{\url{https://pypi.python.org/pypi/bigcode-fetcher}}
\footnote{\url{https://pypi.python.org/pypi/bigcode-embeddings}}.
%
\subsection{Embedding generation dataset}
As described in Section~\ref{sec:token-representation}, data generation for
token embedding is unsupervised, as opposed to the approach we take for clone
detection. Getting data to train our model is therefore much easier. To train
embedding for a particular programming language, the only thing we need is a
large quantity of code written in this given language. This code should also be
as much as possible diverse so we can capture the tokens of most major libraries
and get a descent vector representation for their tokens.

To generate datasets to train token embedding for Python and Java, we fetched
code from GitHub using our \lstinline{bigcode-embeddings} tool, and then
generated the ASTs for all the files using \lstinline{bigcode-astgen}.
For Java, we chose to use all the projects written in Java and belonging to The
Apache Software Foundation\footnote{\url{http://www.apache.org/}}. For Python,
as we could not find any organization with a sufficient amount of source code,
we projects which fulfilled the following conditions.

\begin{itemize}
\item Size between 100KB and 100MB
\item Non-viral license (e.g. MIT, BSD)
\item Not forks
\end{itemize}

We ordered the results by number of stars as a proxy of the popularity of the
project and kept all the files which contained more than 10 tokens and less than
10000 tokens. Although our AST generation tool supports both Python 2 and Python
3, the produced AST may vary slightly and we therefore decided to use only
Python 3 for this experiment.

We show some metrics for our Java dataset in
table~\ref{tab:java-embedding-dataset} and for our Python dataset in
table~\ref{tab:python-embedding-dataset}.

\begin{table}
  \caption{\label{tab:java-embedding-dataset}Embedding generation Java dataset
    metrics}
  \begin{center}
    \begin{tabular}{r r}
      \toprule
      Metric & Value\\
      \toprule
      Projects count & 1,027\\
      Files count & 476,685\\
      Lines count & 80,367,840\\
      Tokens count & 301,930,231\\
      \bottomrule
    \end{tabular}
  \end{center}
\end{table}

\begin{table}
  \caption{\label{tab:python-embedding-dataset}Embedding generation Python
    dataset metrics}
  \begin{center}
    \begin{tabular}{r r}
      \toprule
      Metric & Value\\
      \toprule
      Projects count & 879\\
      Files count & 131,506\\
      Lines count & 55,796,594\\
      Tokens count & 89,757,436\\
      \bottomrule
    \end{tabular}
  \end{center}
\end{table}

\subsection{Vocabulary generation results}
As discussed in Section~\ref{sec:token-representation}, the first step when
creating token embedding is to generate a vocabulary for the given programming
language. We will here present the results for the different vocabularies we
generated. For both Java and Python, we generated 3 different kinds of
vocabularies:

\begin{enumerate}
\item Vocabulary without token values
\item Vocabulary of $500$ tokens with values
\item Vocabulary of $10000$ tokens with values
\end{enumerate}

The vocabulary without token values only contains only the type of each token,
for example, \lstinline{ImportStmt} in Java, or \lstinline{FunctionDef} in
Python. In figures~\ref{fig:no-values-vocabulary-distribution}
to~\ref{fig:with-values-vocabulary-distribution}, we show the relation between
the rank of a token in the vocabulary, and its number of appearances. We use a
log scale for both axes. An interesting property we find is that, when using
values of the tokens, this relation and by extension the distribution of the
frequency tokens in a programming language follows the Zipf's
law~\cite{piantadosi2013zipf}, in the same way as natural languages do.
When we exclude the tokens, and therefore only use the type of each token to
generate the vocabulary, the law seems to hold only until the few last tokens of
each language, which indicates that there is a small number of types in each
language that are almost never used. Regardless of the fact that we use or not
values of each token, the distribution is almost the same for both Python and
Java, and the fact that the curve for Java is above the one for Python comes
from the fact that we extracted more tokens for Java and we did not normalize
the results, as it was not needed for the rest of the embedding generation.

Given the distribution of the token frequency in both programming languages
follow closely the frequency found in natural languages, we can expect that a
model such as skipgram which works well with natural languages to also work
here.

\begin{figure}
  \centering\includegraphics[width=14cm]{images/distribution-without-values.png}
  \caption{\label{fig:no-values-vocabulary-distribution}Tokens without value
    relation between token frequency and rank}
\end{figure}

\begin{figure}
  \centering\includegraphics[width=14cm]{images/distribution-with-values.png}
  \caption{\label{fig:with-values-vocabulary-distribution}Tokens with value
    relation between token frequency and rank}
\end{figure}

\subsection{Embedding generation experiments}
Once we created our vocabulary, we want to assign a vector in $\mathbb{R}^{d_w}$
to each token in our vocabulary. Given the purpose of generating these embedding
is to encode a program so that we can detect clone, the main requirement of our
embedding is that tokens which are semantically similar, such that for example
\lstinline{while} and \lstinline{for} have a small distance, while unrelated
tokens have a large distance.

\begin{table}
  \caption{\label{tab:embedding-hyper-parameters} Experiment settings for
    embedding generation}
  \begin{center}
    \begin{tabularx}{\linewidth}{c X l}
      \toprule
      Parameter & Description & Sample value\\
      \toprule
      \lstinline{vocabulary} & The vocabulary to use for generating the
      embeddings & \lstinline{no-id.tsv}\\
      \lstinline{ancestors_window_size} & The window size for the number of
      ancestors & $2$\\
      \lstinline{descendants_window_size} & The window size for the number of
      descendants & $1$\\
      \lstinline{include_siblings} & Whether or not to include siblings &
      \lstinline{true}\\
      \lstinline{embeddings_size} & The dimension $d_w$ of the embeddings & $100$\\
      \bottomrule
    \end{tabularx}
  \end{center}
\end{table}

In table~\ref{tab:embedding-hyper-parameters}, we describe the different
settings of our experiment. We only show the most important
for the task of generate the embedding and leave out settings such as the batch
size, the optimizer or the learning rate.

Given the unsupervised nature of the model, it is difficult to evaluate
quantitatively the quality of the results. While some methods exist to evaluate
word embedding in natural languages context~\cite{schnabel2015eval}, the
technique uses the nature of the language and is therefore not directly
applicable to programming languages. We are not aware of such a method to
evaluate embedding in programming languages context. We therefore decided to
evaluate quantitatively the results we obtained for the embedding creation task
by manually inspecting the relation between different tokens, and judging of the
quality of the embedding using these relationship. In particular, when training
embedding for vocabulary where token do not have values, we tried to cluster the
tokens using k-means\cite{Kanungo:2002:EKC:628329.628801} and looked at how much
the clusters made sense --- for example, are statements and expressions in
different clusters.

We tried to train the embedding using a large set of values for the different
parameters we had. We tried window sizes from $0$ to $5$ for the ancestors, from
$0$ to $4$ for the descendants, we tried to use siblings and we tried embedding
sizes of $10$, $20$, $50$, $100$ and $200$. When increasing the size of the
embedding did not present any significant benefit, we kept the smaller size. In
table~\ref{tab:no-value-embedding-params}, we show the parameters we found to
work best for training embedding for vocabulary containing values and vocabulary
with types only.

\begin{table}
  \caption{\label{tab:no-value-embedding-params}Embedding experiment optimal settings}
  \begin{center}
    \begin{tabularx}{\linewidth}{X X X}
      \toprule
      Parameter & Vocab without values & Vocab with values\\
      \toprule
      \lstinline{ancestors_window_size} & $2$ & $2$\\
      \lstinline{descendants_window_size} & $1$ & $1$\\
      \lstinline{include_siblings} & \lstinline{false} & \lstinline{false}\\
      \lstinline{embeddings_size} & $50$ & $100$\\
      \bottomrule
    \end{tabularx}
  \end{center}
\end{table}

Increasing the window size too much seems to create too much noise, and did not
yield better results. Likewise, we suspect that including the siblings generated
more noise than signal when training our model. In
figure~\ref{fig:python-embeddings} and~\ref{fig:java-embeddings}, we show the
results we obtained when projecting the trained embeddings in a 2 dimensions
space using PCA\cite{DBLP:journals/corr/Shlens14}. We only plotted a subset of
the points in the vocabulary to keep the figure readable.

\begin{figure}
  \centering\includegraphics[height=9cm]{images/java-embeddings.png}
  \caption{\label{fig:java-embeddings}Java clustered embedding 2D projection for vocabulary without
  values}
\end{figure}

\begin{figure}
  \centering\includegraphics[height=9cm]{images/python-embeddings.png}
  \caption{\label{fig:python-embeddings}Python clustered embedding 2D projection for vocabulary without
  values}
\end{figure}

There are a few interesting things to notice in the results we obtained in
figures~\ref{fig:python-embeddings} and~\ref{fig:java-embeddings}. First, the
clustering we obtain seems quite close to what we could expect when thinking
about the different types of tokens present in the AST. For example, for Java,
All the statements are part of the same cluster, and the only expression which
belongs to this cluster is the \lstinline{LambdaExpr}, which was introduced
in recent versions of Java and appears way less often than most other tokens,
which could explain why it was not clustered correctly. The expressions are also
mostly in the same cluster, and literal expressions, such as
\lstinline{IntegerLiteralExpr} and \lstinline{StringLiteralExpr} are close to
each other, while other related tokens such as \lstinline{ThisExpr} and
\lstinline{SuperExpr} are in the same cluster but further away in the vector
space. The results are similar for Python, where most language constructs, such
as \lstinline{While} or \lstinline{Try} are clustered together, while all the
comparison operators are in another cluster.
Another interesting point is the difference in the relationship of the tokens
between both programming languages. In particular, while \lstinline{While} and
\lstinline{For} appear almost at the same point in the vector space for Java,
which means that both tokens are very similar, the same two tokens are
much further away in Python. This seems a reasonable result to obtain as Java's
\lstinline{while} and \lstinline{for} are indeed interchangeable, while in
Python, these two constructs have different semantics.
