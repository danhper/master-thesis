\chapter{\label{ch:experiments}Experiments}
In this chapter, we will describe the experiments we made and present the
results we obtained. In Section~\ref{sec:clone-detection-experiments}, we will
present our experiments about code clone detection, while in
Section~\ref{sec:token-generation-experiments} we will describe our experiments
to generate token embedding. In both sections we will show what kind of data we
used to train our model and how our model perform with different
hyper-parameters.
\section{\label{sec:clone-detection-experiments}Code clone detection}
To evaluate our clone detection model, we implemented the model we described
in~\ref{sec:clone-detection}, we collected data of programs implementing the
same functionality to feed to the model, and we trained and evaluated our model
with different hyper-parameters. In this section, we will give some details
about the data we collected for our experiments and present the different
results we obtained when evaluating our model.
\subsection{\label{ssec:clone-detection-dataset}Code clone detection dataset}
As we are using a supervised learning approach to code clone detection, to train
our model we needed data which fulfills the following properties.

\begin{enumerate}
\item Multiple code fragments should implement the same functionality
\item Information on whether two code fragments implement the same functionality
  must be included
\item Dataset should contain code fragments written in at least 2 programming
  languages
\end{enumerate}

To the best of our knowledge, no dataset currently available fulfills all the
necessary properties to our experiments, therefore we created our own dataset.

For this dataset, we found that competitive programming websites are an
excellent match. The solution to a single problem is implemented by a
large number of persons in many different languages. All the solutions to a
single problem must implement exactly the same functionality, therefore,
we are assured that all source codes implementing a solution to the same problem
are at least type IV code clones. The easier the problem is, the higher the
probability of code fragments implementing the solution to the same problem have
to be very similar to each other, and to therefore be closer to type III clones.
Furthermore, multiple solutions to a problem are always implemented by different
users, which makes our dataset closer to the motivating example we presented in
Section~\ref{sec:motivating-exmaple}.

To create the dataset, we scraped code from a famous Japanese competitive
programming website. As our implementation currently only supports Java and
Python, we fetched data for these two programming languages. We restricted the
data to only programs that were accepted by the website judging system --- meaning
that the programs actually implemented the solution to the given problem --- in
order to have the type IV code clone guarantee. In
table~\ref{tab:clone-dataset-overview}, we give some statistics about the
dataset we created and in table~\ref{tab:clone-dataset-languages} we show how
the given statistics are distributed between Python and Java. We show the
distribution of the number of files per problem --- which is representative of
the number of clones in the dataset --- in
figure~\ref{fig:files-per-problem-distribution}.

\begin{table}
  \caption{\label{tab:clone-dataset-overview} Clone detection dataset overview}
  \begin{center}
    \begin{tabular}{c c}
      Measure & Value\\
      \toprule
      Problems count & 576\\
      Avg. files / problem & 77\\
      Files count & 44620\\
      Lines count & 1270599\\
      Tokens count & 8554476
    \end{tabular}
  \end{center}
\end{table}

\begin{table}
  \caption{\label{tab:clone-dataset-languages} Clone detection dataset language overview}
  \begin{center}
    \begin{tabular}{c c c}
      & Python & Java\\
      \toprule
      Avg. files / problem & 41 & 36\\
      Files count & 23792 & 20828\\
      Lines count & 312353 & 958246\\
      Tokens count & 1810085 & 6744391
    \end{tabular}
  \end{center}
\end{table}

\begin{figure}
  \begin{center}
    \includegraphics[width=16cm]{./images/code-clone-dataset-problems-distribution.png}
    \caption{\label{fig:files-per-problem-distribution} Distribution of the
      number of files per problem}
  \end{center}
\end{figure}

In order to be able to feed the code to our model, we transform the code we
fetched using \lstinline{bigcode-astgen} we presented in
Section~\ref{sec:ast-generation} using the commands shown in
listing~\ref{lis:atcoder-astgen}.

\begin{lstlisting}[numbers=none,language={},
  caption=AST generation using \lstinline{bigcode-astgen}, label=lis:atcoder-astgen]
bigcode-astgen-java --batch -f 'src/**/*.java' -o java-asts
bigcode-astgen-py --batch -f 'src/**/*.py' -o python-asts
cat java-asts.json python-asts.json > asts.json
cat java-asts.txt python-asts.txt > asts.txt
\end{lstlisting}
With the first two commands, we generate the ASTs for Python and Java source
files, and with the next we commands we merge the ASTs and the file which maps
the index of the AST in the JSON file to the name of the file in the original
dataset.
\subsection{\label{ssec:clone-detection-hyper-params}Code clone detection model hyper-parameters}
Our model contains many hyper-parameters that can vastly influence its
performances while detecting clones. In listing~\ref{lis:model-config}, we show a
sample configuration file that we actually use to train our model. The file is
written using in YAML and is loaded by our system when training and evaluating
the model. Although the file we show here is not complete, it contains the main
settings that one might want to change when training a model.

\begin{figure}
  \lstinputlisting[language={}, basicstyle=\ttfamily\footnotesize, caption=Model
  configuration file,label=lis:model-config]{./snippets/config.yml}
\end{figure}

We describe the meaning of each parameter in our configuration file in
table~\ref{tab:hyper-parameters}.

\begin{table}
  \caption{\label{tab:hyper-parameters}Clone detection model hyper-parameters}
  \begin{center}
    \begin{tabularx}{\linewidth}{c X l}
      \toprule
      Parameter & Description & Sample value\\
      \toprule
      \lstinline{transformer_class_name} & The algorithm to used to encode
      the AST into a vector as described in~\ref{sssec:ast-transformer} & \lstinline{DFSTransformer}\\
      \lstinline{vocabulary} & The path to the extracted vocabulary & \lstinline{vocab.tsv}\\
      \lstinline{embeddings} & The path to the trained token embedding. If
      this parameter is not provided, embedding are randomly initialized & \lstinline{embeddings.pickle}\\
      \lstinline{embeddings_dimension} & The dimension $d_w$ of the trained
      embedding for the target programming language as described
      in~\ref{sssec:token-embedding-layer} & $100$\\
      \lstinline{output_dimensions} & The output dimension $d_e$ of the
      LSTM encoder described in~\ref{sssec:lstm-encoder}. If the length of the array
      is greater than $1$, the LSTM will be stacked and the output dimension will be
      the value of the last element of the list & \lstinline{[100, 50]}\\
      \lstinline{bidirectional_encoding} & Whether or not to use a
      bidirectional LSTM & \lstinline{true}\\
      \lstinline{name} & The name of the programming language & java\\
      \lstinline{input_length} & The maximum number of tokens per code
      fragment. This restriction can be avoided at the cost of a performance
      penalty & $1000$\\
      \lstinline{merge_mode} & The algorithm to use to merge the two code
      fragments as described in~\ref{sssec:merge-layer} & bidistance\\
      \lstinline{merge_output_dim} & The output dimension to use for the
      merge layer. Currently only effective when using
      equation~\ref{eq:bidistance-merge-layer} as the merge layer & \lstinline{50}\\
      \lstinline{dense_layers} & The number of hidden layers to use for the
      final MLP and their number of units & \lstinline{[50, 50]}\\
      \lstinline{optimizer} & The optimizer to use to train the model & rmsprop\\
      \lstinline{submissions_path} & The path of the training dataset
      submissions metadata & \lstinline{submissions.json}\\
      \lstinline{asts_path} & The path of the training data ASTs generated
      as described above & \lstinline{asts.json}\\
      \lstinline{split_ratio} & The amount of data to use for training,
      hyper-parameter tuning, and testing the model & \lstinline{[0.8, 0.1, 0.1]}\\
    \end{tabularx}
  \end{center}
\end{table}
\begin{table}
  \begin{center}
    \begin{tabularx}{\linewidth}{c X l}
      \lstinline{shuffle} & Whether or not to shuffle the data when training
      the model & \lstinline{true}\\
      \lstinline{negative_samples} & The number of non-clone noise data ---
      negative sample --- to generate per code clone & $4$\\
      \lstinline{negative_sample_distance} & The maximum distance ratio
      between the size of the input and the size of the negative sample, to avoid
      the model to try to detect clones by number of tokens & $0.2$\\
      \lstinline{class_weights} & The weight to assign to positive and
      negative samples. The higher the weight, the more an error will be
      penalized & \lstinline{\{0: 2.0, 1: 1.0\}}\\
      \lstinline{batch_size} & The size of a batch when training the model & $256$\\
      \lstinline{epochs} & The number of epochs for which to train the model & $10$\\
      \bottomrule
    \end{tabularx}
  \end{center}
\end{table}
%
Although there is a very large number of hyper-parameters, not all affect our
model in the same way. For example, while the vocabulary we use, or the way we
transform the AST into a vector affects greatly the performance of the model,
other parameters such as the optimizer we use or the batch size do not are not
as important. We will explain more in detail how each section in our
configuration is used, and explain some of the decisions to be made when setting
these parameters.

The parameters under each object of the \lstinline{languages} section contains
the necessary information to encode an AST into a vector for the given
programming language. The vocabulary and embedding we pass will determine how
each token is encoded into its vector representation. We will discuss further
how these embedding are trained in
Section~\ref{sec:token-generation-experiments}, but the most important decision
being made for the embedding is if we want to use the values of the tokens or
simply their types, resulting in a small vocabulary in the order of $100$ tokens
in the first case, or a much larger vocabulary of thousands of tokens in the
second. The input length is the maximum number of tokens for a single code
fragment. We use this limit mainly for performance purpose, as we pad all the
inputs to this size to be able to train our model using batches. Given our
current implementation, removing this limits forces us to train the model a
sample at a time, largely hurting performance. This shortcoming can be mitigated
with techniques such as bucketing~\cite{DBLP:journals/corr/abs-1708-05604}, but
we leave this for future work. The \lstinline{output_dimensions} and
\lstinline{bidirectional_encoding} parameters control what kind of LSTM network
will be used to encode the vectorized AST into a single vector.

Other parameters under \lstinline{model} contain hyper-parameters on how the
input vectors should be merged, as well as what kind of network to use to
actually make the prediction. It also contains information about the optimizer
to use, and may have hyper-parameters for the optimizer such as its learning
rate.

The part under \lstinline{generator} contains all the information necessary to
generate data to train the model. \lstinline{submissions_path} and
\lstinline{asts_path} are the paths to the submissions metadata and their
encoded AST and all the data to train and evaluate the model will be loaded from
their. The negative samples related settings as well as the class weights mostly
affect the precision-recall trade-off, which we will discuss more in detail
in~\ref{ssec:clone-detection-experiment}.
\subsection{\label{ssec:clone-detection-experiment}Clone detection experiment}
In this section, we will describe the experiments we run to evaluate our clone
detection model.
\section{\label{sec:token-generation-experiments}Token embedding generation}
To generate token vector embedding and evaluate our results, we implemented the
model and tools we described in Section~\ref{sec:token-representation}. All the
tools implemented to generated token embedding are publicly available on
GitHub\footnote{\url{https://github.com/tuvistavie/bigcode-tools}}, and made
available through pip when
possible\footnote{\url{https://pypi.python.org/pypi/bigcode-fetcher}}
\footnote{\url{https://pypi.python.org/pypi/bigcode-embeddings}}.
%
\subsection{Embedding generation dataset}
As described in Section~\ref{sec:token-representation}, data generation for
token embedding is unsupervised, as opposed to the approach we take for clone
detection. Getting data to train our model is therefore much easier. To train
embedding for a particular programming language, the only thing we need is a
large quantity of code written in this given language. This code should also be
as much as possible diverse so we can capture the tokens of most major libraries
and get a descent vector representation for their tokens.

To generate datasets to train token embedding for Python and Java, we fetched
code from GitHub using our \lstinline{bigcode-embeddings} tool, and then
generated the ASTs for all the files using \lstinline{bigcode-astgen}.
For Java, we chose to use all the projects written in Java and belonging to The
Apache Software Foundation\footnote{\url{http://www.apache.org/}}. For Python,
as we could not find any organization with a sufficient amount of source code,
we projects which fulfilled the following conditions.

\begin{itemize}
\item Size between 100KB and 100MB
\item Non-viral license (e.g. MIT, BSD)
\item Not forks
\end{itemize}

We ordered the results by number of stars as a proxy of the popularity of the
project and kept all the files which contained more than 10 tokens and less than
10000 tokens. Although our AST generation tool supports both Python 2 and Python
3, the produced AST may vary slightly and we therefore decided to use only
Python 3 for this experiment.

We show some metrics for our Java dataset in
table~\ref{tab:java-embedding-dataset} and for our Python dataset in
table~\ref{tab:python-embedding-dataset}.

\begin{table}
  \caption{\label{tab:java-embedding-dataset}Embedding generation Java dataset
    metrics}
  \begin{center}
    \begin{tabular}{r r}
      \toprule
      Metric & Value\\
      \toprule
      Projects count & 1,027\\
      Files count & 476,685\\
      Lines count & 80,367,840\\
      Tokens count & 301,930,231\\
      \bottomrule
    \end{tabular}
  \end{center}
\end{table}

\begin{table}
  \caption{\label{tab:python-embedding-dataset}Embedding generation Python
    dataset metrics}
  \begin{center}
    \begin{tabular}{r r}
      \toprule
      Metric & Value\\
      \toprule
      Projects count & 879\\
      Files count & 131,506\\
      Lines count & 55,796,594\\
      Tokens count & 89,757,436\\
      \bottomrule
    \end{tabular}
  \end{center}
\end{table}

\subsection{Vocabulary generation results}
As discussed in Section~\ref{sec:token-representation}, the first step when
creating token embedding is to generate a vocabulary for the given programming
language. We will here present the results for the different vocabulary we
generated. For both Java and Python, we generated 3 different kinds of
vocabularies:

\begin{enumerate}
\item Vocabulary without token values
\item Vocabulary of $500$ tokens with values
\item Vocabulary of $10000$ tokens with values
\end{enumerate}
