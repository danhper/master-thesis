\chapter{Our proposal}
In this chapter, we propose a cross-language code clone detection system based
on supervised machine learning. Our system can learn to detect clones across
programming languages by learning from existing code clones pairs, and is
language agnostic in the sense that we use a normalized AST structure which can
be generated from any programming language using a parser into feed our system.
We also present how to generate the data that our system accepts as input to
detect clones, and describe the dataset that we created in order to train and
evaluate our system.
%
\section{Overview}
As discussed in the shortcomings of current approaches to code clone detection
in~\ref{sssec:shortcomings}, current comparison-based approaches to code clone
detection do not fit cross-language clone detection, especially when the target
languages do not share a common intermediate representation.
To overcome this issue, we propose a supervised approach to code clone detection
where the system is not given directly information about how code fragments
should be compared to each other, but rather uses training data to learn what
kind of code fragments should be, or not, considered as clones.

Our system is composed of the following subsystems, which we will describe more
thoroughly in the following sections.
\begin{enumerate}
\item\label{it:sup-model} CC-Learner --- Supervised ML model to learn and predict code clones
\item\label{it:unsup-model} \lstinline{bigcode-embeddings} --- Unsupervised ML
  model to learn token vector representations
\item\label{it:bigcode-astgen} \lstinline{bigcode-astgen} --- Parser modules to
  transform source code to a common AST format
\item\label{it:bigcode-ast-tools} \lstinline{bigcode-ast-tools} --- Tools to transform and
  work with ASTs generated by \lstinline{bigcode-astgen}.
\item\label{it:sup-scraper} CC-Fetcher --- Module to scrape data to train CC-Learner
\item\label{it:unsup-scraper} \lstinline{bigcode-fetcher} --- Module to scrape data to train \lstinline{bigcode-embeddings}
\end{enumerate}
Figure~\ref{fig:system-overview} shows a general overview of how our system
works.

\begin{figure}
\caption{\label{fig:system-overview}Overview of the system}
\end{figure}

The core of our system is~\ref{it:sup-model}, which is an LSTM --- presented
in~\ref{ssec:rnn} --- base supervised machine learning model which takes two
code fragments as an input, and predict if the two code fragments are code
clones. The flow in training mode is to take two code fragments scraped
using the scraper module~\ref{it:sup-scraper}, transform the code pairs to ASTs
using the parser modules~\ref{it:bigcode-astgen}, transform the ASTs in to
vectors using the token representation learned by~\ref{it:unsup-model} and
finally to feed the result to our LSTM-based model~\ref{it:sup-model}.

In Section~\ref{sec:clone-detection}, we will describe in detail the model we
used to detect code clones. In Section~\ref{sec:token-representation}
to~\ref{sec:code-clone-data}, we will describe all the steps to fetch, process
and transform the data in order to be able to feed it into our machine learning
model presented in Section~\ref{sec:clone-detection}.
In Section~\ref{sec:preprocessing}, we will describe the pre-processing steps in
order to be able to work with source code written in different programming
languages, in Section~\ref{sec:token-representation}, we will explain how we
assign a vector in $\mathbb{R}^d$ to each token in the source code, and finally
in Section~\ref{sec:code-clone-data} we will describe what kind of data we use
to train our model and why we chose this kind of data.
%
\section{\label{sec:clone-detection}Code clone detection model}
%
\section{\label{sec:preprocessing}Code pre-processing steps}
%
\section{\label{sec:token-representation}Token-level vector representation}
%
\section{\label{sec:code-clone-data}Code clone data for supervised learning}
