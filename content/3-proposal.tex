\chapter{Our proposal}
In this chapter, we propose a cross-language code clone detection system based
on supervised machine learning. Our system can learn to detect clones across
programming languages by learning from existing code clones pairs, and is
language agnostic in the sense that we use a normalized AST structure which can
be generated from any programming language using a parser into feed our system.
We also present how to generate the data that our system accepts as input to
detect clones, and describe the dataset that we created in order to train and
evaluate our system.
%
\section{\label{sec:proposal-overview}Overview}
As discussed in the shortcomings of current approaches to code clone detection
in~\ref{sssec:shortcomings}, current comparison-based approaches to code clone
detection do not fit cross-language clone detection, especially when the target
languages do not share a common intermediate representation.
To overcome this issue, we propose a supervised approach to code clone detection
where the system is not given directly information about how code fragments
should be compared to each other, but rather uses training data to learn what
kind of code fragments should be, or not, considered as clones.

Our system is composed of the following subsystems, which we will describe more
thoroughly in the following sections.
\begin{enumerate}
\item\label{it:sup-model} CC-Learner --- Supervised ML model to learn and predict code clones
\item\label{it:unsup-model} \lstinline{bigcode-embeddings} --- Unsupervised ML
  model to learn token vector representations
\item\label{it:bigcode-astgen} \lstinline{bigcode-astgen} --- Parser modules to
  transform source code to a common AST format
\item\label{it:bigcode-ast-tools} \lstinline{bigcode-ast-tools} --- Tools to
  transform with ASTs generated by \lstinline{bigcode-astgen}
\item\label{it:sup-scraper} CC-Fetcher --- Module to scrape data to train CC-Learner
\item\label{it:unsup-scraper} \lstinline{bigcode-fetcher} --- Module to scrape data to train \lstinline{bigcode-embeddings}
\end{enumerate}
Figure~\ref{fig:system-overview} shows a general overview of how our system
works.

\begin{figure}
\caption{\label{fig:system-overview}Overview of the system}
\end{figure}

The core of our system is~\ref{it:sup-model}, which is an LSTM --- presented
in~\ref{ssec:rnn} --- base supervised machine learning model which takes two
code fragments as an input, and predict if the two code fragments are code
clones. The flow in training mode is to take two code fragments scraped
using the scraper module~\ref{it:sup-scraper}, transform the code pairs to ASTs
using the parser modules~\ref{it:bigcode-astgen}, transform the ASTs in to
vectors using the token representation learned by~\ref{it:unsup-model} and
finally to feed the result to our LSTM-based model~\ref{it:sup-model}.

In Section~\ref{sec:clone-detection}, we will describe in detail the model we
used to detect code clones. In Section~\ref{sec:token-representation}
to~\ref{sec:code-clone-data}, we will describe all the steps to fetch, process
and transform the data in order to be able to feed it into our machine learning
model presented in Section~\ref{sec:clone-detection}.
In Section~\ref{sec:preprocessing}, we will describe the pre-processing steps in
order to be able to work with source code written in different programming
languages, in Section~\ref{sec:token-representation}, we will explain how we
assign a vector in $\mathbb{R}^d$ to each token in the source code, and finally
in Section~\ref{sec:code-clone-data} we will describe what kind of data we use
to train our model and why we chose this kind of data.
%
\section{\label{sec:clone-detection}Code clone detection model}
\subsection{Model overview}
The clone detection model, CC-Learner, that we first introduced in
Section~\ref{sec:proposal-overview} is the main of our system. Its main role is
to actually predict if two code fragments are, or not, clones. The model is
composed of the following components.
\begin{enumerate}
\item An AST to vector transformer
\item An encoder for AST vectors
\item A merge layer to concatenate the two code fragments vectors
\item A feed-forward neural network to make the code clone prediction
\end{enumerate}
%
Figure~\ref{fig:clone-detection-model} shows how these parts are connected
together.
%
\begin{figure}
\caption{\label{fig:clone-detection-model}CC-Learner code clone detection model}
\end{figure}
%
In this section, we will first describe in details the computations done by the
model. We will then explain how we train the model, then how use the trained
model to predict if two code fragments are clones, and finally we will discuss
about the different hyper parameters of the models.
%
\subsection{Model computations}
We will here formalize the computations each component of the model executes to
produce the final prediction.
\subsubsection{AST to vector transformer}
As input, the model receives an AST, which is a tree structure. The AST to
vector transformer takes this AST and maps it into a vector of integers, where
each integer represents the index of the token in the AST in the vocabulary of
the programming language the source code is written in.

More formally, if we let $\mathcal{T}_l$ be the set of all possible ASTs for
source code written in programming language $l$, the AST transformer $f_l$ will
be a function with the following definition.

\begin{equation}
  f_l : \mathcal{T}_l \rightarrow \mathbb{N}^m
\end{equation}

where $m$ may vary depending on the size of the input tree and the algorithm
used for $f_l$. Furthermore, given $V_l$ is the vocabulary of tokens for the
programming language $l$, the following equation must hold for any
implementation of $f_l$.

\begin{equation}
  \forall t \in \mathcal{T}_l, n \in f_l(t), ~ n \in V_l
\end{equation}
%
For CC-Learner, we provide two different instances of this function, and the
function which is used is a hyper parameter of our model. We will use the code
snippet~\ref{lis:sample-snippet}, the AST of figure~\ref{fig:sample-ast} and the
vocabulary of table~\ref{tab:sample-vocabulary} to illustrate how both of our
instances work.
%
\begin{figure}[tb]
\begin{lstlisting}[language=Java,label=lis:sample-snippet,caption=Sample code snippet]
if (a < 1) {
  return 0;
}
\end{lstlisting}
\end{figure}
%
\begin{figure}[tb]
  \begin{center}
    \includedot[height=5cm]{./diagrams/sample-snippet.dot}
    \caption{\label{fig:sample-ast} Sample AST}
  \end{center}
\end{figure}
%
\begin{table}
  \centering
  \begin{tabular}{cc}
    Token & Index\\
    \toprule%
    \ttfamily{if} & 1\\
    identifier & 2\\
    integer & 3\\
    \ttfamily{<} & 4\\
    \ttfamily{return} & 5\\
    \bottomrule%
  \end{tabular}
  \caption{\label{tab:sample-vocabulary} Sample vocabulary}
\end{table}

The first instance of our AST to vector transformer is very simple. The idea is
simply to traverse the tree using a depth-first, pre-order tree traversal. At
each node in the tree, we look up the index of the node token in the vocabulary,
and we append the index to the output vector.

In our example above, the algorithm will start at the \lstinline{if} node, it
will lookup the index of the node in the vocabulary, which in this case is $1$,
and append $1$ to the output vector. It will then do the same thing with the
other nodes in the following order: \lstinline{<}, \lstinline{a}, \lstinline{1},
\lstinline{return}, \lstinline{0}. After completion, we will therefore obtain
the following vector in $\mathbb{N}^6$.
\[ \begin{pmatrix}1 & 4 & 2 & 3 & 5 & 3\end{pmatrix} \]
When using this algorithms, the output dimension is always the same as the
number of nodes in the input AST.

Although the first instance of our AST to vector transformer is very simple and
straightforward, the main disadvantage is that it completely loses information
about the topology of the input tree. To mitigate this issue, we also
implemented another algorithm which tries to partially preserve the topology of
the tree in the output vector. In the above example, \lstinline{if} has two
children nodes: \lstinline{<} and \lstinline{return}, therefore the distance
inside the tree topology between \lstinline{if} and both of these nodes is $1$.
However, in the output vector, the distance between \lstinline{if} and
\lstinline{<} is $1$, but the distance between \lstinline{if} and
\lstinline{return} is $4$. For such a simple case, the distance is still short
enough for the neural to be able to capture relationship between these elements,
but in more complex cases, a distance of $1$ inside the tree topology could
easily become extremely large. This is the main issue we try to tackle with our
other instance of the AST to vector transformer function. The idea is to perform
a depth-first, pre-order traversal of the tree and generate the same vector as
in the first instance, but also to traverse the tree once more this time using a
depth-first, pre-order traversal but selecting the nodes from right to left
instead of selecting them from left to right when executing the traversal. Using
the above example, this means that in the second traversal, after visiting
\lstinline{if}, instead of visiting \lstinline{<}, we visit \lstinline{return}.
The second traversal order would therefore be \lstinline{if},
\lstinline{return}, \lstinline{0}, \lstinline{<}, \lstinline{1}, \lstinline{a}.
Once we have finished traversing the AST in both direction, we concatenate the
vectors obtain by each traversal to obtain the final vector for the AST. For the
above example, the final vector will be

\[ \begin{pmatrix}1 & 4 & 2 & 3 & 5 & 3 & 1 & 5 & 3 & 4 & 3 & 2\end{pmatrix} \]

As we are concatenating two vectors generated by a full traversal of the tree,
and which will therefore each have the dimension of the number of nodes in the
tree, the final output vector will have twice the dimension of the number of
nodes in the tree. Therefore, if there are $n$ nodes in the tree, the output
will be a vector in $\mathbb{N}^{2n}$.
\section{\label{sec:preprocessing}Code pre-processing steps}
%
\section{\label{sec:token-representation}Token-level vector representation}
%
\section{\label{sec:code-clone-data}Code clone data for supervised learning}
