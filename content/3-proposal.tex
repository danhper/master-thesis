\chapter{Our proposal}
In this chapter, we propose a cross-language code clone detection system based
on supervised machine learning. Our system can learn to detect clones across
programming languages by learning from existing code clones pairs, and is
language agnostic in the sense that we use a normalized AST structure which can
be generated from any programming language using a parser into feed our system.
We also present how to generate the data that our system accepts as input to
detect clones, and describe the dataset that we created in order to train and
evaluate our system.
%
\section{\label{sec:proposal-overview}Overview}
As discussed in the shortcomings of current approaches to code clone detection
in~\ref{sssec:shortcomings}, current comparison-based approaches to code clone
detection do not fit cross-language clone detection, especially when the target
languages do not share a common intermediate representation.
To overcome this issue, we propose a supervised approach to code clone detection
where the system is not given directly information about how code fragments
should be compared to each other, but rather uses training data to learn what
kind of code fragments should be, or not, considered as clones.

Our system is composed of the following subsystems, which we will describe more
thoroughly in the following sections.
\begin{enumerate}
\item\label{it:sup-model} CC-Learner --- Supervised ML model to learn and predict code clones
\item\label{it:unsup-model} \lstinline{bigcode-embeddings} --- Unsupervised ML
  model to learn token vector representations
\item\label{it:bigcode-astgen} \lstinline{bigcode-astgen} --- Parser modules to
  transform source code to a common AST format
\item\label{it:bigcode-ast-tools} \lstinline{bigcode-ast-tools} --- Tools to
  transform with ASTs generated by \lstinline{bigcode-astgen}
\item\label{it:sup-scraper} CC-Fetcher --- Module to scrape data to train CC-Learner
\item\label{it:unsup-scraper} \lstinline{bigcode-fetcher} --- Module to scrape data to train \lstinline{bigcode-embeddings}
\end{enumerate}
Figure~\ref{fig:system-overview} shows a general overview of how our system
works.

\begin{figure}
  \centering\includegraphics[width=13cm]{./images/system-overview.pdf}
  \caption{\label{fig:system-overview}Overview of the system}
\end{figure}

The core of our system is~\ref{it:sup-model}, which is an LSTM --- presented
in~\ref{ssec:rnn} --- base supervised machine learning model which takes two
code fragments as an input, and predict if the two code fragments are code
clones. The flow in training mode is to take two code fragments scraped
using the scraper module~\ref{it:sup-scraper}, transform the code pairs to ASTs
using the parser modules~\ref{it:bigcode-astgen}, transform the ASTs in to
vectors using the token representation learned by~\ref{it:unsup-model} and
finally to feed the result to our LSTM-based model~\ref{it:sup-model}.

In Section~\ref{sec:clone-detection}, we will describe in detail the model we
used to detect code clones. In Section~\ref{sec:token-representation}
to~\ref{sec:code-clone-data}, we will describe all the steps to fetch, process
and transform the data in order to be able to feed it into our machine learning
model presented in Section~\ref{sec:clone-detection}.
In Section~\ref{sec:preprocessing}, we will describe the pre-processing steps in
order to be able to work with source code written in different programming
languages, in Section~\ref{sec:token-representation}, we will explain how we
assign a vector in $\mathbb{R}^d$ to each token in the source code, and finally
in Section~\ref{sec:code-clone-data} we will describe what kind of data we use
to train our model and why we chose this kind of data.
%
\section{\label{sec:clone-detection}Code clone detection model}
\subsection{Model overview}
The clone detection model, CC-Learner, that we first introduced in
Section~\ref{sec:proposal-overview} is the main of our system. Its main role is
to actually predict if two code fragments are, or not, clones. The model is
composed of the following components.
\begin{enumerate}
\item An AST to vector transformer
\item A token-level embedding layer
\item An LSTM-based encoder for AST vectors
\item A merge layer to concatenate the two code fragments vectors
\item A feed-forward neural network to make the code clone prediction
\end{enumerate}
%
Figure~\ref{fig:clone-detection-model} shows how these parts are connected
together.
%
\begin{figure}
  \centering\includegraphics[width=16cm]{./images/model-overview.pdf}
  \caption{\label{fig:clone-detection-model}CC-Learner code clone detection model}
\end{figure}
%
In this section, we will first describe in details the computations done by the
model. We will then explain how we train the model, then how use the trained
model to predict if two code fragments are clones, and finally we will discuss
about the different hyper parameters of the models.
%
\subsection{Model computations}
We will here formalize the computations each component of the model executes to
produce the final prediction.
\subsubsection{AST to vector transformer}
As input, the model receives an AST, which is a tree structure. The AST to
vector transformer takes this AST and maps it into a vector of integers, where
each integer represents the index of the token in the AST in the vocabulary of
the programming language the source code is written in.

More formally, if we let $\mathcal{T}_l$ be the set of all possible ASTs for
source code written in programming language $l$, the AST transformer $f_t^l$ will
be a function with the following definition.

\begin{equation}
  f_t^l : \mathcal{T}_l \rightarrow \mathbb{N}^m
\end{equation}

where $m$ may vary depending on the size of the input tree and the algorithm
used for $f_t^l$. Furthermore, given $V_l$ is the vocabulary of tokens for the
programming language $l$, the following equation must hold for any
implementation of $f_t^l$.

\begin{equation}
  \forall t \in \mathcal{T}_l, n \in f_t^l(t), ~ n \in V_l
\end{equation}
%
For CC-Learner, we provide two different instances of this function, and the
function which is used is a hyper parameter of our model. We will use the code
snippet~\ref{lis:sample-snippet}, the AST of figure~\ref{fig:sample-ast} and the
vocabulary of table~\ref{tab:sample-vocabulary} to illustrate how both of our
instances work.
%
\begin{figure}
\begin{lstlisting}[language=Java,label=lis:sample-snippet,caption=Sample code snippet]
if (a < 1) {
  return 0;
}
\end{lstlisting}
\vspace*{1cm}
\end{figure}
%
\begin{figure}
  \centering\includedot[height=5cm]{./diagrams/sample-snippet.dot}
  \vspace*{-1cm}
  \caption{\label{fig:sample-ast} Sample AST}
\end{figure}
%
\begin{table}
  \centering
  \begin{tabular}{cc}
    Token & Index\\
    \toprule%
    \ttfamily{if} & 1\\
    identifier & 2\\
    integer & 3\\
    \ttfamily{<} & 4\\
    \ttfamily{return} & 5\\
    \bottomrule%
  \end{tabular}
  \caption{\label{tab:sample-vocabulary} Sample vocabulary}
\end{table}

The first instance of our AST to vector transformer is very simple. The idea is
simply to traverse the tree using a depth-first, pre-order tree traversal. At
each node in the tree, we look up the index of the node token in the vocabulary,
and we append the index to the output vector.

In our example above, the algorithm will start at the \lstinline{if} node, it
will lookup the index of the node in the vocabulary, which in this case is $1$,
and append $1$ to the output vector. It will then do the same thing with the
other nodes in the following order: \lstinline{<}, \lstinline{a}, \lstinline{1},
\lstinline{return}, \lstinline{0}. After completion, we will therefore obtain
the following vector in $\mathbb{N}^6$.
\begin{equation}
  \label{eq:sample-indexes-vector}
  \begin{pmatrix}1 & 4 & 2 & 3 & 5 & 3\end{pmatrix}
\end{equation}
When using this algorithm, the output dimension is always the same as the
number of nodes in the input AST.

Although the first instance of our AST to vector transformer is very simple and
straightforward, the main disadvantage is that it completely loses information
about the topology of the input tree. To mitigate this issue, we also
implemented another algorithm which tries to partially preserve the topology of
the tree in the output vector. In the above example, \lstinline{if} has two
children nodes: \lstinline{<} and \lstinline{return}, therefore the distance
inside the tree topology between \lstinline{if} and both of these nodes is $1$.
However, in the output vector, the distance between \lstinline{if} and
\lstinline{<} is $1$, but the distance between \lstinline{if} and
\lstinline{return} is $4$. For such a simple case, the distance is still short
enough for the neural to be able to capture relationship between these elements,
but in more complex cases, a distance of $1$ inside the tree topology could
easily become extremely large. This is the main issue we try to tackle with our
other instance of the AST to vector transformer function. The idea is to perform
a depth-first, pre-order traversal of the tree and generate the same vector as
in the first instance, but also to traverse the tree once more this time using a
depth-first, pre-order traversal but selecting the nodes from right to left
instead of selecting them from left to right when executing the traversal. Using
the above example, this means that in the second traversal, after visiting
\lstinline{if}, instead of visiting \lstinline{<}, we visit \lstinline{return}.
The second traversal order would therefore be \lstinline{if},
\lstinline{return}, \lstinline{0}, \lstinline{<}, \lstinline{1}, \lstinline{a}.
Once we have finished traversing the AST in both direction, we concatenate the
vectors obtain by each traversal to obtain the final vector for the AST. For the
above example, the final vector will be

\[ \begin{pmatrix}1 & 4 & 2 & 3 & 5 & 3 & 1 & 5 & 3 & 4 & 3 & 2\end{pmatrix} \]

As we are concatenating two vectors generated by a full traversal of the tree,
and which will therefore each have the dimension of the number of nodes in the
tree, the final output vector will have twice the dimension of the number of
nodes in the tree. Therefore, if there are $n$ nodes in the tree, the output
will be a vector in $\mathbb{N}^{2n}$.
%
\subsubsection{Token-level embedding layer}
When we transform an AST in to a vector using the above approach, we obtain a
vector of indexes. As each index cannot be fed directly to the encoder, the two
main approaches are to use a one-hot vector for the index, or to use a
distributed vector representation. As using a distributed vector representation
has many advantages~\cite{DBLP:journals/corr/MikolovSCCD13} over using one-hot
vectors, we choose this approach for our model. We will here assume that we
already have the embedding for each token available, and will describe how we
actually compute these embedding in Section~\ref{sec:token-representation}.

The embedding layer of the model transforms a vector in $\mathbb{N}^n$ into a
matrix in $\mathbb{R}^{n\times d_w}$ where $d_w$ is the dimension of the embedding
and is a hyper parameter of the model. The embedding layer for a programming
language $l$ is therefore a function $f_w^l$ with the following dimensions.

\begin{equation}
  f_w^l : \mathbb{N}^n \rightarrow \mathbb{R}^{n\times d_w}
\end{equation}

Such embedding are actually represented by a matrix $W^l$ which has dimensions
of $\mathbb{R}^{|V|\times d_w}$ where $|V|$ is the size programming language $l$
vocabulary, and $d_2$ is the dimension of the embedding.
Sample values for embedding of dimension $d_w = 3$ are show in
table~\ref{tab:sample-vocabulary}.
The sample embedding given in table~\ref{tab:sample-vocabulary} are
shown in their matrix form in equation~\ref{eq:sample-vocabulary}.

\begin{equation}
  \label{eq:sample-vocabulary}
  W^l =
  \begin{pmatrix}
    0.12 & -0.43 & 0.66\\
    -0.81 & 0.01 & 0.28\\
    0.91 & 0.33 & 0.47\\
    -0.51 & -0.27 & -0.09\\
    0.17 & 0.73 & -0.56
  \end{pmatrix}
\end{equation}

Transforming an index $i$ in its vector representation therefore reduces to
selecting the $i$th index of the matrix $W^l$, and the function $f_w$ can
therefore be expressed as in equation~\ref{eq:embedding-func}, and is
computationally inexpensive. In \ref{eq:embedding-func}, $W_i$ means selecting
the $i$th row of the matrix $W$.

\begin{equation}
  \label{eq:embedding-func}
  f_w^l(i) = W_i^l
\end{equation}

Given the embedding of dimension $d_w = 3$ given in table~\ref{tab:sample-embedding}
for the sample vocabulary presented in table~\ref{tab:sample-vocabulary}, the
vector of indexes in equation~\ref{eq:sample-indexes-vector} will be transformed
in the matrix of equation~\ref{eq:sample-embbed-matrix}.

\begin{table}
  \centering
  \begin{tabular}{cl}
    Index & Embedding\\
    \toprule
    $1$ & $\begin{pmatrix}0.12 & -0.43 & 0.66\end{pmatrix}$\\
    $2$ & $\begin{pmatrix}-0.81 & 0.01 & 0.28\end{pmatrix}$\\
    $3$ & $\begin{pmatrix}0.91 & 0.33 & 0.47\end{pmatrix}$\\
    $4$ & $\begin{pmatrix}-0.51 & -0.27 & -0.09\end{pmatrix}$\\
    $5$ & $\begin{pmatrix}0.17 & 0.73 & -0.56\end{pmatrix}$\\
  \end{tabular}
  \caption{\label{tab:sample-embedding} Sample embedding of dimension $3$}
\end{table}

\begin{equation}
  \label{eq:sample-embbed-matrix}
  \begin{pmatrix}
    0.12 & -0.43 & 0.66\\
    -0.51 & -0.27 & -0.09\\
    -0.81 & 0.01 & 0.28\\
    0.91 & 0.33 & 0.47\\
    0.17 & 0.73 & -0.56\\
    0.91 & 0.33 & 0.47
  \end{pmatrix}
\end{equation}

In the matrix shown in equation~\ref{eq:sample-embbed-matrix}, each row is the
embedding of the index in the original vector.
%
\subsubsection{LSTM-based encoder}
As discussed previously, the output of the embedding layer will be a matrix of
dimension $\mathbb{R}^{n\times d}$ where $d$ where $n$ depends on the size of
the input AST and $d$ is the dimension of the embedding, which is a
hyper-parameter of the model. The purpose of the LSTM-based encoder layer is to
transform this matrix into a vector in $d_e$, where $d_e$ is a hyper-parameter of
the model, which captures the relation between the elements in the input matrix.
This task is the equivalent of transforming a matrix representing a sentence to
a vector representing the same sentence for natural language processing.
This layer will therefore be a function $f_e$ defined as in
equation~\ref{eq:lstm-encoder}.

\begin{equation}
  \label{eq:lstm-encoder}
  f_e : \mathbb{R}^{n\times d} \rightarrow \mathbb{R}^{d_e}
\end{equation}

An interesting property of this function is that the output dimension is
independent of the input dimension, which makes it possible to easily aggregate
inputs which originally had different dimensions in the next layer of the model.

To achieve this task, we use an LSTM model as described in~\ref{ssec:rnn}, and
feed it the result of the embedding layer. $d_e$ is chosen experimentally, and is
usually between $20$ and $200$ depending on the other hyper-parameters of the
model.

Given the result shown in equation~\ref{eq:sample-embbed-matrix} and $d_e = 4$,
equation~\ref{eq:sample-lstm-output} shows a sample output of the LSTM encoder.

\begin{equation}
  \label{eq:sample-lstm-output}
  \begin{pmatrix}
    0.81 & -0.12 & 0.77 & -0.45
  \end{pmatrix}
\end{equation}

There are also other choices that need to be made when instaciating the
LSTM. We first need to decide if the LSTM will be bi-directional or not. When
the LSTM is bi-directional, there will actually be two LSTMs: an LSTM for the
forward direction of the output of the embedding layer, and an LSTM for the
reverse direction. After the two computations run, the result of the two LSTMs
will be concatenated to give the final output. The final dimension of the vector
will therefore be $2d_e$ instead of $d_e$. Equation~\ref{eq:sample-bilstm-output}
shows a sample output for equation~\ref{eq:sample-embbed-matrix} as input and
$d_e = 4$ when using a bi-directional LSTM.

\begin{equation}
  \label{eq:sample-bilstm-output}
  \begin{pmatrix}
    0.81 & -0.12 & 0.77 & -0.45 & -0.68 & 0.22 & 0.94 & -0.03
  \end{pmatrix}
\end{equation}

Another decision to make for the LSTM-based encoder is whether to use a single
LSTM which works as described in~\ref{ssec:rnn}, or to stack multiple LSTMs
above one another. When stacking multiple LSTMs, the first LSTM works exactly a
before, however, its output is not directly used but fed to another LSTM and
only the uppermost LSTM result is used for the next step. We try to use 1 to 3
stacked LSTM and chose the best value for our use case experimentally.
%
\subsubsection{Merge layer}
Up to know, we have focused on how to encode a single AST into a vector in
$\mathbb{R}^{d_e}$. However, our model takes two inputs, and produces a single
output. We therefore need to combine these two inputs in some way that allows us
to generate a single prediction to whether or not the two given inputs are
code clones or not. This is the role of merge layer in our model. It takes two
inputs and return a single output. The merge layer is therefore defined by a
function $f_m$ as shown in equation~\ref{eq:merge-layer}, where $n$ normally
depends on $d_e$.

\begin{equation}
  \label{eq:merge-layer}
  f_m : \mathbb{R}^{d_e} \times \mathbb{R}^{d_e} \rightarrow \mathbb{R}^{n}
\end{equation}

In our model, we define two different types of merge layers which we will
describe below.

The first instance of the merge layer is used as a baseline, and is a simple
concatenation operation. We take the vector outputted by the LSTM-based encoder
for the two input ASTs, and concatenate them into a single vector in
$\mathbb{R}^{2d_e}$. In this case, we therefore have $n = 2d_e$.
For example, given the LSTM output given in equation~\ref{eq:sample-lstm-output}
and another LSTM output given by equation~\ref{eq:sample-lstm-output-2},

\begin{equation}
  \label{eq:sample-lstm-output-2}
  \begin{pmatrix}0.12 & 0.86 & -0.27 & 0.33\end{pmatrix}
\end{equation}

the merge layer would output the following vector.

\begin{equation}
  \begin{pmatrix}
    0.81 & -0.12 & 0.77 & -0.45 & 0.12 & 0.86 & -0.27 & 0.33
  \end{pmatrix}
\end{equation}

The second instance of the merge layer uses the combination of the angle and the
distance of the two vectors described in~\cite{DBLP:journals/corr/TaiSM15}.
Given $h_L$ the output of the encoder for the first code snippet and $h_R$ the
output of the encoder for the second code snippet, the output $h$ of the merge
layer is defined by equation~\ref{eq:bidistance-merge-layer}.

\begin{align}
  \label{eq:bidistance-merge-layer}
  h_\times &= h_L \odot h_R\\
  h_+ &= |h_L - h_R| \nonumber\\
  h &= W^{(\times)}h_\times + W^{(+)}h_+ \nonumber
\end{align}

When using this merge strategy, $W^{(\times)}$ and $W^{(+)}$ are parameters of
the model and are both matrices with dimension $\mathbb{R}^{d_o\times d_e}$
where $d_o$ is a hyper-parameter of the model. The dimensions of the different
results will therefore be $h_\times \in \mathbb{R}^{d_e}$, $h_+ \in
\mathbb{R}^{d_e}$ and $h \in \mathbb{R}^{d_o}$. Using the outputs given in
equations~\ref{eq:sample-lstm-output} and~\ref{eq:sample-lstm-output-2}, we will
therefore obtain the following results.

\begin{align*}
  h_\times &= \begin{pmatrix}0.0972 & -0.1032 & -0.2079 & -0.1485\end{pmatrix}\\
  h_+ &= \begin{pmatrix}0.69 & 0.98 & 1.04 & 0.78\end{pmatrix}
\end{align*}
$h$ will then be obtain by doing a dot product using the weights of the model
and the results shown above.
\subsubsection{Feed-forward neural network}
Once we obtain a single vector containing information about the two input code
snippets, we use a feed-forward neural network to predict if the inputs were
code clones or not. The output layer of our neural network uses a sigmoid
function defined as in~\ref{eq:sigmoid}, and therefore outputs a real number
between $0$ and $1$ which can be interpreted as a probability.

\begin{equation}
  \label{eq:sigmoid}
  \sigma(x) = \frac{1}{1 + e^{-x}}
\end{equation}

The feed-forward neural network can therefore be represented by a function $f_p$
defined as follow.

\begin{equation}
  \label{eq:feed-forward}
  f_p : \mathbb{R}^{d_o} \rightarrow [0, 1]
\end{equation}

This layer has the usual hyper-parameters found in feed-forward neural networks,
for example, the number of hidden layers, the number of units per layer and the
number or the activation function of the hidden layers. We choose these
hyper-parameters experimentally.
\section{\label{sec:preprocessing}Code pre-processing steps}
%
\section{\label{sec:token-representation}Token-level vector representation}
%
\section{\label{sec:code-clone-data}Code clone data for supervised learning}
